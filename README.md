# HGTDR + TransE: Drug Repurposing with Heterogeneous Graph Transformers and Knowledge Graph Embeddings

This repository provides an implementation based on the HGTDR paper, extended to incorporate knowledge graph embeddings (KGEs) generated by TransE alongside node features from BioBERT and ChemBERTa. The goal is to leverage complementary structural information from the knowledge graph topology for improved drug repurposing predictions.

## Introduction: Drug Repurposing

Developing new drugs is a costly and time-consuming process with high failure rates. Drug repurposing, which involves finding new therapeutic uses for existing approved drugs, offers a promising alternative to accelerate drug development, reduce costs, and lower risks. Computational methods, particularly those leveraging large-scale biomedical knowledge graphs (KGs), have emerged as powerful tools for identifying potential drug repurposing candidates by analyzing complex relationships between drugs, diseases, proteins, genes, and other biological entities.

## About HGTDR (Original Paper)

This work builds upon the research presented in:

*   **Title:** HGTDR: Advancing drug repurposing with heterogeneous graph transformers
*   **DOI:** [10.1093/bioinformatics/btae349](https://doi.org/10.1093/bioinformatics/btae349)
*   **Original Code:** https://github.com/bcb-sut/HGTDR

The HGTDR paper proposed a novel approach for drug repurposing using Heterogeneous Graph Transformers (HGT) on the PrimeKG biomedical knowledge graph.

## Modifications in this Repository (HGTDR + TransE)

This repository extends the original HGTDR implementation with the primary goal of enhancing the node feature representation by incorporating structural information learned directly from the graph topology.

**Key Changes:**

1.  **TransE Embeddings:**
    *   A standard Knowledge Graph Embedding model, TransE, is trained on the PrimeKG dataset using the [PyKEEN](https://github.com/pykeen/pykeen) library (see `src/PYKEENtrans.py`). TransE learns low-dimensional vector representations for entities and relations by modeling relationships as translations in the embedding space (`head + relation â‰ˆ tail`).
    *   The pre-trained TransE *entity* embeddings are saved.
2.  **Entity Identifier Alignment:**
    *   The PyKEEN training uses the display names of entities (e.g., 'Aspirin', 'Diabetes Mellitus') as identifiers.
    *   The HGTDR preprocessing script (`src/wtranse.ipynb`) has been modified to use these same display names when building its internal graph structure and entity dictionaries, ensuring correct lookup into the TransE embeddings. 
    *   A reverse mapping (display name -> `type::index`) is created internally to look up BioBERT/ChemBERTa embeddings, which expect the `type::index` format.
3.  **Combined Feature Vectors:**
    *   The TransE entity embeddings are concatenated with the BioBERT and ChemBERTa embeddings during the preprocessing step.
    *   The input linear layer of the HGT model is adjusted to handle this new, larger combined feature dimension.
4.  **Framework Alignment:**
    *   This implementation adheres to the 5-fold cross-validation structure used in the original paper, loading the same pre-defined data splits (`train[1-5].pkl`, `val[1-5].pkl`) for fair comparison.
    *   The batching logic (`make_batch`, `make_test_batch`) and loss calculation (`compute_loss`) have been carefully aligned with the original implementation after debugging.

The hypothesis is that the TransE embeddings capture global graph structure and relational patterns that might be complementary to the context-based features from BioBERT/ChemBERTa, potentially leading to improved prediction performance.

## Installation

1.  **Install Dependencies:**
    *   **Create `requirements.txt`:** You should create a `requirements.txt` file listing necessary packages. 
        ```txt
        torch
        torch_geometric
        pandas
        numpy
        scikit-learn
        matplotlib
        pykeen
        pickle 
        ```

    *   **Install PyTorch Geometric:** Follow instructions on the [PyG website](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html), ensuring compatibility with your PyTorch version.
    *   **Install Other Packages:**
        ```bash
        pip install -r requirements.txt
        ```

## Data Requirements

Place the following files/directories in the specified locations relative to the main project directory:

1.  **`../data/kg.csv`**: The PrimeKG triples file used for graph construction and BioBERT/ChemBERTa lookups.
2.  **`../data/entities_embeddings.pkl`**: Pre-computed BioBERT embeddings (expecting `id` column with `type::index` format).
3.  **`../data/smiles_embeddings.pkl`**: Pre-computed ChemBERTa embeddings for drugs (expecting `id` column with `drug::index` format).
4.  **`../data/CV data/`**: Directory containing the 5-fold cross-validation splits:
    *   `train1.pkl`, `train2.pkl`, ..., `train5.pkl`
    *   `val1.pkl`, `val2.pkl`, ..., `val5.pkl`
    (These files should contain pickled PyG `HeteroData` objects representing the splits).
5.  **TransE Embeddings & Mapping (Generate these first!):**
    *   **`primekg_transe_embeddings_with_mapping/entity_embeddings.npy`**: Generated by `src/PYKEENtrans.py`.
    *   **`primekg_transe_embeddings_with_mapping/entity_to_id_mapping.pkl`**: Generated by `src/PYKEENtrans.py`. Contains the mapping from display names to PyKEEN's internal IDs.

**IMPORTANT:** You MUST run the `src/PYKEENtrans.py` script first to train TransE and generate the `.npy` and `.pkl` files required for the main HGTDR+TransE script.

## Usage

1.  **Generate TransE Embeddings & Mapping:**
    *   Navigate to the `src` directory (or adjust paths in the script).
    *   Run the PyKEEN training script:
        ```bash
        python PYKEENtrans.py
        ```
    *   This will create the `primekg_transe_embeddings_with_mapping` directory containing `entity_embeddings.npy` and `entity_to_id_mapping.pkl`.

2.  **Run HGTDR + TransE:**
    *   The main logic is provided in `src/wtranse.py`. It's recommended to convert this to a Jupyter Notebook (`.ipynb`) for easier execution and inspection of outputs cell-by-cell.
    *   Ensure the file paths in **Cell 2 (Configuration)** are correct, especially `TRANSE_EMBEDDINGS_PATH` and `PYKEEN_MAPPING_PATH`.
    *   Run the cells sequentially in the notebook.
    *   The script will perform the 5-fold cross-validation.

3.  **Output:**
    *   Logs will be printed to the console and saved to files in the `OUT_DIR` (default: `../out_combined_5fold/`), separated by fold (e.g., `Fold_1_out.txt`).
    *   Loss plots for each fold will be saved (e.g., `Fold_1_final_losses.png`).
    *   A summary file (`summary_results.txt`) and a detailed CSV (`detailed_results.csv`) containing the aggregated AUROC and AUPR across the 5 folds will be saved in `OUT_DIR`.

## Configuration

Key parameters can be modified in **Cell 2 (Configuration)** of the main script/notebook:

*   **Embedding Selection:** `USE_BIOBERT`, `USE_CHEMBERTA`, `USE_TRANSE` (boolean flags). Set `USE_TRANSE` to `False` to run without TransE embeddings for comparison.
*   **HGTDR Hyperparameters:** Found in the `HGTDR_CONFIG` dictionary (e.g., `batch_size`, `epochs`, `learning_rate`, `hidden_channels`, `dropout`, etc.).
*   **Output Directory:** `OUT_DIR`.
*   **GPU Usage:** The `get_device_for_fold` function implements a simple strategy. Modify if you have a different number of GPUs or prefer manual assignment.
