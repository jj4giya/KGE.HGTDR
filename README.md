# HGTDR + TransE: Drug Repurposing with Heterogeneous Graph Transformers and Knowledge Graph Embeddings

This repository provides an implementation based on the HGTDR paper, extended to incorporate knowledge graph embeddings (KGEs) generated by TransE alongside node features from BioBERT and ChemBERTa. The goal is to leverage complementary structural information from the knowledge graph topology for improved drug repurposing predictions.

## Introduction: Drug Repurposing

Developing new drugs is a costly and time-consuming process with high failure rates. Drug repurposing, which involves finding new therapeutic uses for existing approved drugs, offers a promising alternative to accelerate drug development, reduce costs, and lower risks. Computational methods, particularly those leveraging large-scale biomedical knowledge graphs (KGs), have emerged as powerful tools for identifying potential drug repurposing candidates by analyzing complex relationships between drugs, diseases, proteins, genes, and other biological entities.

## About HGTDR (Original Paper)

This work builds upon the research presented in:

*   **Title:** HGTDR: Advancing drug repurposing with heterogeneous graph transformers
*   **DOI:** [10.1093/bioinformatics/btae349](https://doi.org/10.1093/bioinformatics/btae349)
*   **Original Code:** https://github.com/bcb-sut/HGTDR

The HGTDR paper proposed a novel approach for drug repurposing using Heterogeneous Graph Transformers (HGT) on the PrimeKG biomedical knowledge graph.

## Modifications in this Repository (HGTDR + TransE)

This repository extends the original HGTDR implementation with the primary goal of enhancing the node feature representation by incorporating structural information learned directly from the graph topology.

**Key Changes:**

1.  **TransE Embeddings:**
    *   A standard Knowledge Graph Embedding model, TransE, is trained on the PrimeKG dataset using the [PyKEEN](https://github.com/pykeen/pykeen) library (see `src/PYKEENtrans.py`). TransE learns low-dimensional vector representations for entities and relations by modeling relationships as translations in the embedding space (`head + relation â‰ˆ tail`).
    *   The pre-trained TransE *entity* embeddings are saved.
2.  **Entity Identifier Alignment:**
    *   The PyKEEN training uses the display names of entities (e.g., 'Aspirin', 'Diabetes Mellitus') as identifiers.
    *   The HGTDR preprocessing script (`src/wtranse.ipynb`) has been modified to use these same display names when building its internal graph structure and entity dictionaries, ensuring correct lookup into the TransE embeddings. 
    *   A reverse mapping (display name -> `type::index`) is created internally to look up BioBERT/ChemBERTa embeddings, which expect the `type::index` format.
3.  **Combined Feature Vectors:**
    *   The TransE entity embeddings are concatenated with the BioBERT and ChemBERTa embeddings during the preprocessing step.
    *   The input linear layer of the HGT model is adjusted to handle this new, larger combined feature dimension.
4.  **Framework Alignment:**
    *   This implementation adheres to the 5-fold cross-validation structure used in the original paper, loading the same pre-defined data splits (`train[1-5].pkl`, `val[1-5].pkl`) for fair comparison.
    *   The batching logic (`make_batch`, `make_test_batch`) and loss calculation (`compute_loss`) have been carefully aligned with the original implementation after debugging.

The hypothesis is that the TransE embeddings capture global graph structure and relational patterns that might be complementary to the context-based features from BioBERT/ChemBERTa, potentially leading to improved prediction performance.

## Installation

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/jj4giya/KGE.HGTDR
    cd KGE.HGTDR
    ```

2.  **Create Conda Environment:**
    This project uses Conda for managing dependencies. An `environment.yml` file is provided to recreate the exact environment used for development and testing.
    *   **Prerequisite:** Ensure you have Anaconda or Miniconda installed.
    *   **Create Environment:** Open your terminal, navigate to the cloned repository directory (where `environment.yml` is located), and run the following command:
        ```bash
        conda env create -f environment.yml
        ```
        This command will create a new Conda environment with the name specified inside the `environment.yml` file (`MPHGT`) and install all the necessary packages listed within it.
    *   **Activate Environment:** Once the environment is created, activate it using:
        ```bash
        conda activate MPHGT
        ```
        You should now see the environment name (`(MPHGT)`) at the beginning of your terminal prompt.

**Important Note on PyTorch/PyG/CUDA Compatibility:**

This project requires **PyTorch Geometric (PyG)**. PyG versions have specific compatibility requirements with PyTorch and CUDA versions. While the included `environment.yml` provides a baseline, **you must ensure the installed PyTorch and PyG versions are compatible with your specific hardware (especially your CUDA toolkit version).**

Please refer to the official **PyTorch Geometric Installation Guide** for the latest compatibility details and installation commands:
[Link 1](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)
or
[Link 2](https://pytorch-geometric.com/whl/)

You may need to reinstall PyTorch and PyG after creating the environment from the YAML file if the default versions are not suitable for your system (particularly for GPU usage).

Now you have the necessary environment set up to run the project code.

## Data Requirements

Download data from : [Link](https://dml.ir/HGTDR_data)

Place the following files/directories in the specified locations relative to the main project directory:

1.  **`../data/kg.csv`**: The PrimeKG triples file used for graph construction and BioBERT/ChemBERTa lookups.
2.  **`../data/entities_embeddings.pkl`**: Pre-computed BioBERT embeddings (expecting `id` column with `type::index` format).
3.  **`../data/smiles_embeddings.pkl`**: Pre-computed ChemBERTa embeddings for drugs (expecting `id` column with `drug::index` format).
4.  **`../data/CV data/`**: Directory containing the 5-fold cross-validation splits:
    *   `train1.pkl`, `train2.pkl`, ..., `train5.pkl`
    *   `val1.pkl`, `val2.pkl`, ..., `val5.pkl`
    (These files should contain pickled PyG `HeteroData` objects representing the splits).
5.  **TransE Embeddings & Mapping (Generate these first!):**
    *   **`data/entity_embeddings.npy`**: Generated by `src/PYKEENtrans.py`.
    *   **`data/entity_to_id_mapping.pkl`**: Generated by `src/PYKEENtrans.py`. Contains the mapping from display names to PyKEEN's internal IDs.

## Usage

1.  **Generate TransE Embeddings & Mapping:**
    *   Navigate to the `src` directory (or adjust paths in the script).
    *   Run the PyKEEN training script:
        ```bash
        python PYKEENtrans.py
        ```
    *   This will create the `primekg_transe_embeddings_with_mapping` directory containing `entity_embeddings.npy` and `entity_to_id_mapping.pkl`.

2.  **Run HGTDR + TransE:**
    *   The main logic is provided in `src/wtranse.ipynb`.
    *   Ensure the file paths in **Cell 2 (Configuration)** are correct, especially `TRANSE_EMBEDDINGS_PATH` and `PYKEEN_MAPPING_PATH`.
    *   Run the cells sequentially in the notebook.
    *   The script will perform the 5-fold cross-validation.

3.  **Output:**
    *   Logs will be printed to the console and saved to files in the `OUT_DIR` (default: `../out_combined_5fold/`), separated by fold (e.g., `Fold_1_out.txt`).
    *   Loss plots for each fold will be saved (e.g., `Fold_1_final_losses.png`).
    *   A summary file (`summary_results.txt`) and a detailed CSV (`detailed_results.csv`) containing the aggregated AUROC and AUPR across the 5 folds will be saved in `OUT_DIR`.

## Configuration

Key parameters can be modified in **Cell 2 (Configuration)** of the main script/notebook:

*   **Embedding Selection:** `USE_BIOBERT`, `USE_CHEMBERTA`, `USE_TRANSE` (boolean flags). Set `USE_TRANSE` to `False` to run without TransE embeddings for comparison.
*   **HGTDR Hyperparameters:** Found in the `HGTDR_CONFIG` dictionary (e.g., `batch_size`, `epochs`, `learning_rate`, `hidden_channels`, `dropout`, etc.).
*   **Output Directory:** `OUT_DIR`.
*   **GPU Usage:** The `get_device_for_fold` function implements a simple strategy. Modify if you have a different number of GPUs or prefer manual assignment.
