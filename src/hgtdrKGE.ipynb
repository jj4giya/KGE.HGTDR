{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ec84ad",
   "metadata": {},
   "source": [
    "# HGTDR + TransE Embeddings - 5-Fold CV Comparison\n",
    "\n",
    "This notebook modifies the original HGTDR implementation\n",
    "to incorporate TransE embeddings (loaded externally) alongside BioBERT and\n",
    "ChemBERTa, followed by the implementation of the paper's 5-fold CV structure for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218b982",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aa3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "from torch_geometric.loader import HGTLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f90f12",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79970ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- File Paths ---\n",
    "PRIMKG_CSV_PATH = '../data/kg.csv'\n",
    "BIOBERT_EMBEDDINGS_PATH = '../data/entities_embeddings.pkl'\n",
    "CHEMBERTA_EMBEDDINGS_PATH = '../data/smiles_embeddings.pkl'\n",
    "TRANSE_EMBEDDINGS_PATH = '../data/entity_embeddings.npy' # Adjust as needed\n",
    "PYKEEN_MAPPING_PATH = '../data/entity_to_id_mapping.pkl'\n",
    "CV_DATA_DIR = '../data/CV data/'\n",
    "OUT_DIR = '../output'\n",
    "\n",
    "# --- Embedding Dimensions ---\n",
    "BIOBERT_DIM = 768\n",
    "CHEMBERTA_DIM = 768\n",
    "TRANSE_DIM = 50 # From TransE training\n",
    "\n",
    "# --- Embedding Selection ---\n",
    "USE_BIOBERT = True\n",
    "USE_CHEMBERTA = True\n",
    "USE_TRANSE = True\n",
    "\n",
    "# --- HGTDR Config (Match hgtdrOG.py) ---\n",
    "HGTDR_CONFIG = {\n",
    "    \"num_samples\": 512,      # Per layer in HGTLoader\n",
    "    \"batch_size\": 164,\n",
    "    \"dropout\": 0.5,          # Dropout in HGT input projection and Predictor\n",
    "    \"epochs\": 300,           \n",
    "    \"hidden_channels\": [64, 64, 64, 64], # Input projection + 3 layers\n",
    "    \"out_channels\": 64,      # GNN output embedding size\n",
    "    \"num_heads\": [8, 8, 8],  # Heads per HGT layer\n",
    "    \"num_layers\": 3,\n",
    "    \"learning_rate\": 0.001,  # Default AdamW LR, adjust if needed\n",
    "    \"weight_decay\": 0.01,    # Default AdamW WD, adjust if needed\n",
    "    \"predictor_dropout\": 0.2 # Specific dropout for the MLP predictor\n",
    "}\n",
    "\n",
    "# --- Node Types and Relation ---\n",
    "NODE_TYPE_DRUG = 'drug'\n",
    "NODE_TYPE_DISEASE = 'disease'\n",
    "INDICATION_RELATION = 'indication'\n",
    "INDICATION_EDGE_TYPE = (NODE_TYPE_DRUG, INDICATION_RELATION, NODE_TYPE_DISEASE)\n",
    "REV_INDICATION_EDGE_TYPE = (NODE_TYPE_DISEASE, INDICATION_RELATION, NODE_TYPE_DRUG) # If exists\n",
    "\n",
    "# --- Device Setup ---\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"Found {NUM_GPUS} GPUs.\")\n",
    "# Simple fold assignment (adjust if more GPUs)\n",
    "def get_device_for_fold(fold_num):\n",
    "    \"\"\" if NUM_GPUS == 0:\n",
    "        print(\"Warning: No GPU found, using CPU.\")\n",
    "        return torch.device('cpu')\n",
    "    elif NUM_GPUS == 1:\n",
    "        return torch.device('cuda:0')\n",
    "    else: # Assign to 2 GPUs\n",
    "        gpu_id = 0 if fold_num <= 3 else 1\n",
    "        return torch.device(f'cuda:{gpu_id}') \"\"\"\n",
    "    print(\"Forcing CPU execution for debugging.\")\n",
    "    return torch.device('cpu')\n",
    "\n",
    "# --- Output Directory ---\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "print(f\"Output directory: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf82e0",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_out(text, filename=\"out.txt\", fold_num=None):\n",
    "    \"\"\"Writes text to a file, optionally creating fold-specific logs.\"\"\"\n",
    "    prefix = f\"Fold_{fold_num}_\" if fold_num is not None else \"\"\n",
    "    filepath = os.path.join(OUT_DIR, prefix + filename)\n",
    "    print(text) # Also print to console\n",
    "    try:\n",
    "        with open(filepath, 'a') as out_file:\n",
    "            out_file.write(str(text) + '\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to {filepath}: {e}\")\n",
    "\n",
    "def plot_losses(losses, val_losses, filename=\"losses.png\", fold_num=None):\n",
    "    \"\"\"Plots losses and saves the figure, optionally fold-specific.\"\"\"\n",
    "    prefix = f\"Fold_{fold_num}_\" if fold_num is not None else \"\"\n",
    "    filepath = os.path.join(OUT_DIR, prefix + filename)\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title(f'Fold {fold_num} - Training and Validation Loss' if fold_num else 'Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(filepath, dpi=200)\n",
    "        plt.close() \n",
    "        print(f\"Saved loss plot to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting/saving losses to {filepath}: {e}\")\n",
    "\n",
    "\n",
    "def calculate_metrics(scores, labels, fold_num):\n",
    "    \"\"\"Calculates and logs AUROC and AUPR.\"\"\"\n",
    "    scores_np = scores.detach().cpu().numpy()\n",
    "    labels_np = labels.detach().cpu().numpy()\n",
    "    \n",
    "    if not np.all(np.isfinite(scores_np)):\n",
    "        print(\"Warning: Non-finite scores detected. Replacing with 0.\")\n",
    "        scores_np = np.nan_to_num(scores_np)\n",
    "    if not np.all(np.isfinite(labels_np)):\n",
    "        print(\"Error: Non-finite labels detected. Cannot calculate metrics.\")\n",
    "        return 0.0, 0.0 # Return default values\n",
    "\n",
    "    # Check for single class in labels\n",
    "    if len(np.unique(labels_np)) < 2:\n",
    "        write_to_out(f\"Warning: Only one class present in labels for Fold {fold_num}. Metrics are undefined.\", fold_num=fold_num)\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    lr_auc = roc_auc_score(labels_np, scores_np)\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(labels_np, scores_np)\n",
    "    lr_aupr = auc(lr_recall, lr_precision)\n",
    "\n",
    "    write_to_out(f'Fold {fold_num} Final Val AUROC: {lr_auc:.4f}', fold_num=fold_num)\n",
    "    write_to_out(f'Fold {fold_num} Final Val AUPR: {lr_aupr:.4f}', fold_num=fold_num)\n",
    "\n",
    "    # Optional: Plot curves (can generate many files)\n",
    "    # plot_metric_curves(scores_np, labels_np, fold_num)\n",
    "\n",
    "    return lr_auc, lr_aupr\n",
    "\n",
    "# Optional function to plot curves if desired\n",
    "def plot_metric_curves(scores_np, labels_np, fold_num):\n",
    "    \"\"\"Plots AUROC and AUPR curves.\"\"\"\n",
    "    try:\n",
    "        # AUROC Curve\n",
    "        ns_probs = [0 for _ in range(len(labels_np))]\n",
    "        ns_fpr, ns_tpr, _ = roc_curve(labels_np, ns_probs)\n",
    "        lr_fpr, lr_tpr, _ = roc_curve(labels_np, scores_np)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label='HGTDR+TransE')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Fold {fold_num} AUROC')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f'Fold_{fold_num}_AUROC.png'), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # AUPR Curve\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(labels_np, scores_np)\n",
    "        no_skill = len(labels_np[labels_np==1]) / len(labels_np)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_recall, lr_precision, marker='.', label='HGTDR+TransE')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Fold {fold_num} AUPR')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f'Fold_{fold_num}_AUPR.png'), dpi=150)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        write_to_out(f\"Error plotting curves for Fold {fold_num}: {e}\", fold_num=fold_num)\n",
    "\n",
    "def compute_loss(scores, labels):\n",
    "    \"\"\"Calculates weighted BCE loss based on batch label distribution.\"\"\"\n",
    "    pos_num = (labels == 1).sum().item()\n",
    "    neg_num = (labels == 0).sum().item()\n",
    "\n",
    "    # Handle cases where one class might be missing in a batch\n",
    "    if pos_num == 0 or neg_num == 0:\n",
    "         # Fallback to unweighted loss if one class is missing\n",
    "         # write_to_out(\"Warning: Only one class present in batch, using unweighted loss.\")\n",
    "         return F.binary_cross_entropy_with_logits(scores, labels.float(), reduction='mean')\n",
    "\n",
    "    # Create weights tensor\n",
    "    pos_weight_val = neg_num / labels.shape[0]\n",
    "    neg_weight_val = pos_num / labels.shape[0]\n",
    "\n",
    "    # Create weights tensor based on labels\n",
    "    weights = torch.zeros_like(labels, dtype=torch.float)\n",
    "    weights[labels == 1] = pos_weight_val\n",
    "    weights[labels == 0] = neg_weight_val\n",
    "\n",
    "    # Calculate weighted loss\n",
    "    weight_for_pos_class = torch.tensor([neg_num / pos_num], device=scores.device) if pos_num > 0 else torch.tensor([1.0], device=scores.device)\n",
    "\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels.float(), pos_weight=weight_for_pos_class)\n",
    "\n",
    "def edge_exists(edges, edge):\n",
    "    \"\"\"Checks if a single edge exists within a set of edges.\"\"\"\n",
    "    # edges: Tensor of shape [2, N]\n",
    "    # edge: Tensor of shape [2, 1] or [2]\n",
    "    if edges is None or edges.numel() == 0:\n",
    "        return False\n",
    "    edge = edge.view(2, 1) # Ensure edge is [2, 1]\n",
    "    edges = edges.to(edge.device)\n",
    "    return (edges == edge).all(dim=0).any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe81f18",
   "metadata": {},
   "source": [
    "## 4. Data Loading (Raw KG and Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ba92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load PrimeKG Raw Data ---\n",
    "try:\n",
    "    df_primekg_raw = pd.read_csv(PRIMKG_CSV_PATH, sep=\",\")\n",
    "    print(f\"Loaded PrimeKG raw data: {df_primekg_raw.shape[0]} triplets\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: PrimeKG file not found at {PRIMKG_CSV_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- Load Base Embeddings (BioBERT/ChemBERTa) ---\n",
    "biobert_embeddings_df = None\n",
    "if USE_BIOBERT:\n",
    "    try:\n",
    "        biobert_embeddings_df = pd.read_pickle(BIOBERT_EMBEDDINGS_PATH)\n",
    "        print(f\"Loaded BioBERT embeddings: {biobert_embeddings_df.shape[0]} entities\")\n",
    "        if 'embedding' in biobert_embeddings_df.columns and len(biobert_embeddings_df) > 0:\n",
    "            inferred_dim = len(biobert_embeddings_df['embedding'].iloc[0])\n",
    "            if inferred_dim != BIOBERT_DIM:\n",
    "                print(f\"Warning: Inferred BioBERT dim ({inferred_dim}) != configured dim ({BIOBERT_DIM}). Using inferred.\")\n",
    "                BIOBERT_DIM = inferred_dim\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: BioBERT embeddings file not found at {BIOBERT_EMBEDDINGS_PATH}. Skipping.\")\n",
    "        USE_BIOBERT = False\n",
    "\n",
    "chemberta_embeddings_df = None\n",
    "if USE_CHEMBERTA:\n",
    "    try:\n",
    "        chemberta_embeddings_df = pd.read_pickle(CHEMBERTA_EMBEDDINGS_PATH)\n",
    "        print(f\"Loaded ChemBERTa embeddings: {chemberta_embeddings_df.shape[0]} drugs\")\n",
    "        if 'embedding' in chemberta_embeddings_df.columns and len(chemberta_embeddings_df) > 0:\n",
    "            inferred_dim = len(chemberta_embeddings_df['embedding'].iloc[0])\n",
    "            if inferred_dim != CHEMBERTA_DIM:\n",
    "                print(f\"Warning: Inferred ChemBERTa dim ({inferred_dim}) != configured dim ({CHEMBERTA_DIM}). Using inferred.\")\n",
    "                CHEMBERTA_DIM = inferred_dim\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: ChemBERTa embeddings file not found at {CHEMBERTA_EMBEDDINGS_PATH}. Skipping.\")\n",
    "        USE_CHEMBERTA = False\n",
    "\n",
    "# --- Load TransE Embeddings ---\n",
    "transe_embeddings_npy = None\n",
    "if USE_TRANSE:\n",
    "    try:\n",
    "        transe_embeddings_npy = np.load(TRANSE_EMBEDDINGS_PATH)\n",
    "        print(f\"Loaded TransE embeddings: shape {transe_embeddings_npy.shape}\")\n",
    "        inferred_dim = transe_embeddings_npy.shape[1]\n",
    "        if inferred_dim != TRANSE_DIM:\n",
    "            print(f\"Warning: Loaded TransE dim ({inferred_dim}) != configured dim ({TRANSE_DIM}). Using loaded.\")\n",
    "            TRANSE_DIM = inferred_dim\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: TransE embeddings file not found at {TRANSE_EMBEDDINGS_PATH}. Skipping.\")\n",
    "        USE_TRANSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343374c3",
   "metadata": {},
   "source": [
    "## 5. PyKEEN Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd724a18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "pykeen_entity_to_id = None\n",
    "if USE_TRANSE:\n",
    "    try:\n",
    "        # Option 1: Load saved mapping (if you saved it during TransE training)\n",
    "        if PYKEEN_MAPPING_PATH and os.path.exists(PYKEEN_MAPPING_PATH):\n",
    "            with open(PYKEEN_MAPPING_PATH, 'rb') as f:\n",
    "                pykeen_entity_to_id = pickle.load(f)\n",
    "            print(f\"Loaded PyKEEN entity mapping from {PYKEEN_MAPPING_PATH}\")\n",
    "        else:\n",
    "            # Option 2: Recreate from PyKEEN dataset (Requires pykeen installed)\n",
    "            try:\n",
    "                from pykeen.datasets import PrimeKG\n",
    "                print(\"Recreating PyKEEN mapping from PrimeKG dataset...\")\n",
    "                pykeen_dataset = PrimeKG()\n",
    "                pykeen_entity_to_id = pykeen_dataset.training.entity_to_id\n",
    "                print(f\"Recreated PyKEEN entity mapping: {len(pykeen_entity_to_id)} entities\")\n",
    "                # Optional: Save the mapping for future use\n",
    "                # with open('pykeen_primekg_entity_mapping.pkl', 'wb') as f:\n",
    "                #     pickle.dump(pykeen_entity_to_id, f)\n",
    "            except ImportError:\n",
    "                print(\"Error: PyKEEN library not installed. Cannot recreate mapping.\")\n",
    "                print(\"Please install PyKEEN or provide a path to a saved mapping (PYKEEN_MAPPING_PATH).\")\n",
    "                USE_TRANSE = False\n",
    "            except Exception as e:\n",
    "                print(f\"Error recreating PyKEEN mapping: {e}. Skipping TransE.\")\n",
    "                USE_TRANSE = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or recreating PyKEEN mapping: {e}\")\n",
    "        USE_TRANSE = False\n",
    "\n",
    "if USE_TRANSE and pykeen_entity_to_id is None:\n",
    "    print(\"Error: Could not load or create PyKEEN mapping. Disabling TransE usage.\")\n",
    "    USE_TRANSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a374b",
   "metadata": {},
   "source": [
    "## 6. Preprocessing Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4374d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_fold_data(fold_num, df_kg, biobert_df, chemberta_df, transe_npy, pykeen_map,\n",
    "                        inference_mode=False): \n",
    "    \"\"\"\n",
    "    Prepares HeteroData objects. If inference_mode=True, only prepares full_data\n",
    "    and skips loading splits and masking. Otherwise, prepares for CV fold.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Preprocessing Fold {fold_num} {'(Inference Mode)' if inference_mode else ''} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Steps 1-5: Build the full_data object (Always Run) ---\n",
    "\n",
    "    # 1. Filter DataFrame\n",
    "    print(\"Filtering nodes based on indication involvement...\")\n",
    "    # ... (Filtering logic unchanged) ...\n",
    "    indication_pairs = df_kg[df_kg['relation'] == INDICATION_RELATION]; drugs_in_indication = set(indication_pairs[indication_pairs['x_type'] == NODE_TYPE_DRUG]['x_index']) | set(indication_pairs[indication_pairs['y_type'] == NODE_TYPE_DRUG]['y_index']); diseases_in_indication = set(indication_pairs[indication_pairs['x_type'] == NODE_TYPE_DISEASE]['x_index']) | set(indication_pairs[indication_pairs['y_type'] == NODE_TYPE_DISEASE]['y_index'])\n",
    "    df_filtered = df_kg[((df_kg['x_type'] != NODE_TYPE_DRUG) | df_kg['x_index'].isin(drugs_in_indication)) & ((df_kg['x_type'] != NODE_TYPE_DISEASE) | df_kg['x_index'].isin(diseases_in_indication)) & ((df_kg['y_type'] != NODE_TYPE_DRUG) | df_kg['y_index'].isin(drugs_in_indication)) & ((df_kg['y_type'] != NODE_TYPE_DISEASE) | df_kg['y_index'].isin(diseases_in_indication))].copy()\n",
    "    print(f\"Filtered DataFrame rows: {len(df_filtered)}\")\n",
    "\n",
    "    # 2. Create Global Entity/Edge Dictionaries (using display names)\n",
    "    print(\"Creating global entity and edge dictionaries (using display names)...\")\n",
    "    entity_dictionary = {}; edge_dictionary = {}; node_type_map = {}\n",
    "    def insert_entry_by_name(display_name, ent_type, entity_dict, type_map):\n",
    "        if ent_type not in entity_dict: entity_dict[ent_type] = {}\n",
    "        if display_name not in entity_dict[ent_type]: entity_dict[ent_type][display_name] = len(entity_dict[ent_type]);\n",
    "        if display_name not in type_map: type_map[display_name] = ent_type\n",
    "        return entity_dict, type_map\n",
    "    valid_triplets_named = []; processed_rows = 0; skipped_rows = 0\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        src_name, src_type = row['x_name'], row['x_type']; dst_name, dst_type = row['y_name'], row['y_type']; relation = row['relation']\n",
    "        if pd.isna(src_name) or pd.isna(dst_name): skipped_rows += 1; continue\n",
    "        entity_dictionary, node_type_map = insert_entry_by_name(src_name, src_type, entity_dictionary, node_type_map); entity_dictionary, node_type_map = insert_entry_by_name(dst_name, dst_type, entity_dictionary, node_type_map)\n",
    "        valid_triplets_named.append((src_name, relation, dst_name)); processed_rows += 1\n",
    "    print(f\"Processed {processed_rows} rows for dictionaries, skipped {skipped_rows} rows with NaN names.\")\n",
    "    for src_name, rel, dst_name in valid_triplets_named:\n",
    "        src_type = node_type_map.get(src_name); dst_type = node_type_map.get(dst_name)\n",
    "        if src_type is None or dst_type is None: continue\n",
    "        src_hgt_id = entity_dictionary[src_type][src_name]; dst_hgt_id = entity_dictionary[dst_type][dst_name]\n",
    "        etype = (src_type, rel, dst_type); pair = (src_hgt_id, dst_hgt_id)\n",
    "        if etype not in edge_dictionary: edge_dictionary[etype] = []\n",
    "        edge_dictionary[etype].append(pair)\n",
    "    print(\"Entity and edge dictionaries created.\")\n",
    "\n",
    "    # 3. Create Base HeteroData Object (`full_data`)\n",
    "    print(\"Creating base HeteroData object...\")\n",
    "    full_data = HeteroData(); node_feature_dims = {}\n",
    "    combined_dim = 0;\n",
    "    if USE_BIOBERT: combined_dim += BIOBERT_DIM\n",
    "    if USE_TRANSE: combined_dim += TRANSE_DIM\n",
    "    for node_type, mapping in entity_dictionary.items():\n",
    "        num_nodes = len(mapping); full_data[node_type].num_nodes = num_nodes; full_data[node_type].global_ids = torch.arange(num_nodes)\n",
    "        final_node_dim = 0\n",
    "        if USE_BIOBERT: final_node_dim += BIOBERT_DIM\n",
    "        if node_type == NODE_TYPE_DRUG and USE_CHEMBERTA: final_node_dim += CHEMBERTA_DIM\n",
    "        if USE_TRANSE: final_node_dim += TRANSE_DIM\n",
    "        node_feature_dims[node_type] = final_node_dim\n",
    "        if final_node_dim > 0: full_data[node_type].x = torch.randn((num_nodes, final_node_dim), dtype=torch.float) * 0.01\n",
    "        else: full_data[node_type].x = None\n",
    "\n",
    "    # 4. Populate Embeddings in `full_data`\n",
    "    print(\"Populating combined embeddings (TransE First)...\")\n",
    "    current_offset = {nt: 0 for nt in entity_dictionary.keys()}\n",
    "    if USE_TRANSE and transe_npy is not None and pykeen_map is not None:\n",
    "        print(\"  Adding TransE...\") # ... (TransE loop) ...\n",
    "        missing_in_pykeen = 0; added_prints_per_type = {nt: 0 for nt in entity_dictionary.keys()}; max_added_prints = 3\n",
    "        for node_type, mapping in entity_dictionary.items():\n",
    "            if full_data[node_type].x is None: continue\n",
    "            count = 0; type_offset = current_offset[node_type]\n",
    "            for display_name, hgt_id in mapping.items():\n",
    "                entity_name_pykeen = display_name\n",
    "                if entity_name_pykeen in pykeen_map:\n",
    "                    pykeen_id = pykeen_map[entity_name_pykeen]\n",
    "                    if 0 <= pykeen_id < transe_npy.shape[0]:\n",
    "                        emb = torch.tensor(transe_npy[pykeen_id], dtype=torch.float)\n",
    "                        if emb.shape[0] == TRANSE_DIM:\n",
    "                            full_data[node_type].x[hgt_id, type_offset : type_offset + TRANSE_DIM] = emb; count += 1\n",
    "                            if added_prints_per_type[node_type] < max_added_prints: print(f\"    OK: Added TransE for {node_type}: '{display_name}' (PyKEEN ID: {pykeen_id})\"); added_prints_per_type[node_type] += 1\n",
    "                else: missing_in_pykeen += 1\n",
    "            current_offset[node_type] += TRANSE_DIM\n",
    "        if missing_in_pykeen > 0: print(f\"    Warning: {missing_in_pykeen} entities (by name) not found in PyKEEN mapping for TransE.\")\n",
    "        else: print(\"    Success: All entities found in PyKEEN mapping for TransE.\")\n",
    "    print(\"  Adding BioBERT...\")\n",
    "    name_to_type_index_map = {}; print(\"    Building name -> type::index map for BioBERT/ChemBERTa...\")\n",
    "    unique_names_df = df_kg[['x_name', 'x_type', 'x_index']].dropna(subset=['x_name']).drop_duplicates(subset=['x_name']); name_to_type_index_map.update({row['x_name']: f\"{row['x_type']}::{row['x_index']}\" for _, row in unique_names_df.iterrows()})\n",
    "    unique_names_df = df_kg[['y_name', 'y_type', 'y_index']].dropna(subset=['y_name']).drop_duplicates(subset=['y_name']); name_to_type_index_map.update({row['y_name']: f\"{row['y_type']}::{row['y_index']}\" for _, row in unique_names_df.iterrows() if row['y_name'] not in name_to_type_index_map})\n",
    "    print(f\"    Built map with {len(name_to_type_index_map)} entries.\")\n",
    "    if USE_BIOBERT and biobert_df is not None: # ... (BioBERT loop) ...\n",
    "        missing_biobert_lookup = 0\n",
    "        for node_type, mapping in entity_dictionary.items():\n",
    "            if full_data[node_type].x is None: continue\n",
    "            count = 0; type_offset = current_offset[node_type]\n",
    "            for display_name, hgt_id in mapping.items():\n",
    "                type_index_key = name_to_type_index_map.get(display_name);\n",
    "                if type_index_key is None: missing_biobert_lookup += 1; continue\n",
    "                biobert_row = biobert_df[biobert_df['id'] == type_index_key]\n",
    "                if not biobert_row.empty:\n",
    "                    emb = torch.tensor(biobert_row['embedding'].iloc[0], dtype=torch.float)\n",
    "                    if emb.shape[0] == BIOBERT_DIM: full_data[node_type].x[hgt_id, type_offset : type_offset + BIOBERT_DIM] = emb; count += 1\n",
    "            current_offset[node_type] += BIOBERT_DIM\n",
    "        if missing_biobert_lookup > 0: print(f\"    Warning: Could not map back {missing_biobert_lookup} display names to type::index for BioBERT.\")\n",
    "    if USE_CHEMBERTA and chemberta_df is not None and NODE_TYPE_DRUG in entity_dictionary: # ... (ChemBERTa loop) ...\n",
    "        print(\"  Adding ChemBERTa...\")\n",
    "        missing_chemberta_lookup = 0; node_type = NODE_TYPE_DRUG; mapping = entity_dictionary[node_type]\n",
    "        if full_data[node_type].x is not None:\n",
    "            count = 0; type_offset = current_offset[node_type]\n",
    "            for display_name, hgt_id in mapping.items():\n",
    "                type_index_key = name_to_type_index_map.get(display_name);\n",
    "                if type_index_key is None: missing_chemberta_lookup += 1; continue\n",
    "                chemberta_row = chemberta_df[chemberta_df['id'] == type_index_key]\n",
    "                if not chemberta_row.empty:\n",
    "                    emb = torch.tensor(chemberta_row['embedding'].iloc[0], dtype=torch.float)\n",
    "                    if emb.shape[0] == CHEMBERTA_DIM: full_data[node_type].x[hgt_id, type_offset : type_offset + CHEMBERTA_DIM] = emb; count += 1\n",
    "            current_offset[node_type] += CHEMBERTA_DIM\n",
    "        if missing_chemberta_lookup > 0: print(f\"    Warning: Could not map back {missing_chemberta_lookup} drug display names to type::index for ChemBERTa.\")\n",
    "\n",
    "\n",
    "    # 5. Add Edges to `full_data`\n",
    "    print(\"Adding edges to base HeteroData...\")\n",
    "    for etype, pairs_list in edge_dictionary.items():\n",
    "        if pairs_list: edge_index = torch.tensor(pairs_list, dtype=torch.long).t().contiguous(); full_data[etype].edge_index = edge_index\n",
    "    print(\"Base HeteroData object created and embeddings populated.\")\n",
    "\n",
    "    # --- Conditional Split Loading and Masking ---\n",
    "    if not inference_mode:\n",
    "        # --- Steps 6 & 7: Only run for actual CV folds ---\n",
    "        # 6. Create Fold-Specific Data (Train/Val) using LOADED HeteroData splits\n",
    "        print(f\"Loading CV split data for Fold {fold_num}...\")\n",
    "        train_split_path = os.path.join(CV_DATA_DIR, f'train{fold_num}.pkl'); val_split_path = os.path.join(CV_DATA_DIR, f'val{fold_num}.pkl')\n",
    "        try:\n",
    "            with open(train_split_path, 'rb') as f: loaded_train_data = pickle.load(f)\n",
    "            with open(val_split_path, 'rb') as f: loaded_val_data = pickle.load(f)\n",
    "            if not isinstance(loaded_train_data, HeteroData) or not isinstance(loaded_val_data, HeteroData): raise TypeError(\"Loaded CV split files are not HeteroData objects.\")\n",
    "            if INDICATION_EDGE_TYPE not in loaded_train_data.edge_types: raise KeyError(f\"Indication edge type {INDICATION_EDGE_TYPE} not found in loaded train data.\")\n",
    "            val_label_edge_type = None\n",
    "            if INDICATION_EDGE_TYPE in loaded_val_data.edge_types and hasattr(loaded_val_data[INDICATION_EDGE_TYPE], 'edge_label_index'): val_label_edge_type = INDICATION_EDGE_TYPE\n",
    "            elif REV_INDICATION_EDGE_TYPE in loaded_val_data.edge_types and hasattr(loaded_val_data[REV_INDICATION_EDGE_TYPE], 'edge_label_index'): val_label_edge_type = REV_INDICATION_EDGE_TYPE; print(f\"Note: Validation labels found in reverse edge type {REV_INDICATION_EDGE_TYPE}. Will flip.\")\n",
    "            else: raise KeyError(f\"Indication edge type {INDICATION_EDGE_TYPE} or its reverse has no 'edge_label_index' in loaded validation data.\")\n",
    "        except Exception as e: print(f\"Error loading or validating CV split files for Fold {fold_num}: {e}\"); return None, None, None, None, None\n",
    "        train_data = copy.deepcopy(full_data); val_data = copy.deepcopy(full_data)\n",
    "        train_indication_edges = loaded_train_data[INDICATION_EDGE_TYPE].edge_index; train_data[INDICATION_EDGE_TYPE].edge_index = train_indication_edges\n",
    "        if REV_INDICATION_EDGE_TYPE in train_data.edge_types: train_data[REV_INDICATION_EDGE_TYPE].edge_index = train_indication_edges[[1, 0], :]\n",
    "        val_indication_edges_to_label = loaded_val_data[val_label_edge_type].edge_label_index\n",
    "        if val_label_edge_type == REV_INDICATION_EDGE_TYPE: val_data[INDICATION_EDGE_TYPE].edge_label_index = val_indication_edges_to_label[[1, 0], :]\n",
    "        else: val_data[INDICATION_EDGE_TYPE].edge_label_index = val_indication_edges_to_label\n",
    "        val_data[INDICATION_EDGE_TYPE].edge_label = torch.ones(val_indication_edges_to_label.shape[1])\n",
    "\n",
    "        # 7. Apply Training Mask\n",
    "        print(\"Applying training mask...\")\n",
    "        num_train_indic = train_data[INDICATION_EDGE_TYPE].edge_index.shape[1]\n",
    "        if num_train_indic == 0: # ... (handle no training edges) ...\n",
    "            print(\"Warning: No training indication edges found for this fold.\")\n",
    "            train_data[INDICATION_EDGE_TYPE]['mask'] = torch.empty(0, dtype=torch.bool); train_data[INDICATION_EDGE_TYPE]['edge_label_index'] = torch.empty((2, 0), dtype=torch.long); train_data[INDICATION_EDGE_TYPE]['edge_label'] = torch.empty(0, dtype=torch.float)\n",
    "            if REV_INDICATION_EDGE_TYPE in train_data.edge_types: train_data[REV_INDICATION_EDGE_TYPE]['mask'] = torch.empty(0, dtype=torch.bool)\n",
    "        else: # ... (apply mask) ...\n",
    "            mask_indices = random.sample(range(num_train_indic), int(num_train_indic * 0.8)); mask = torch.zeros(num_train_indic, dtype=torch.bool); mask[mask_indices] = True\n",
    "            train_data[INDICATION_EDGE_TYPE]['mask'] = mask; train_data[INDICATION_EDGE_TYPE]['edge_label_index'] = train_data[INDICATION_EDGE_TYPE].edge_index[:, mask]; train_data[INDICATION_EDGE_TYPE]['edge_label'] = torch.ones(len(mask_indices))\n",
    "            train_data[INDICATION_EDGE_TYPE].edge_index = train_data[INDICATION_EDGE_TYPE].edge_index[:, ~mask]\n",
    "            if REV_INDICATION_EDGE_TYPE in train_data.edge_types:\n",
    "                if train_data[REV_INDICATION_EDGE_TYPE].edge_index.shape[1] == num_train_indic: train_data[REV_INDICATION_EDGE_TYPE]['mask'] = mask; train_data[REV_INDICATION_EDGE_TYPE].edge_index = train_data[REV_INDICATION_EDGE_TYPE].edge_index[:, ~mask]\n",
    "                else: print(f\"Warning: Size mismatch between forward/reverse indication edges during masking. Skipping reverse edge filtering.\")\n",
    "\n",
    "        print(f\"Fold {fold_num} preprocessing finished in {time.time() - start_time:.2f} seconds.\")\n",
    "        # Return fold-specific data\n",
    "        return train_data, val_data, full_data, entity_dictionary, node_feature_dims\n",
    "\n",
    "    else: # --- inference_mode is True ---\n",
    "        print(f\"Fold {fold_num} preprocessing (Inference Mode) finished in {time.time() - start_time:.2f} seconds.\")\n",
    "        return None, None, full_data, entity_dictionary, node_feature_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfab89",
   "metadata": {},
   "source": [
    "## 7. Model Definitions (HGT + Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c69af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class HGT_Combined(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, dropout, node_feature_dims, metadata):\n",
    "        super().__init__()\n",
    "        assert len(hidden_channels) == num_layers + 1\n",
    "        assert len(num_heads) == num_layers\n",
    "\n",
    "        self.lin_dict = nn.ModuleDict()\n",
    "        for node_type, in_dim in node_feature_dims.items():\n",
    "            if in_dim > 0:\n",
    "                self.lin_dict[node_type] = Linear(in_dim, hidden_channels[0])\n",
    "            else:\n",
    "                 print(f\"Note: Node type '{node_type}' has 0 input dimension. Skipping input linear layer.\")\n",
    "\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            conv = HGTConv(in_channels=hidden_channels[i],\n",
    "                           out_channels=hidden_channels[i+1],\n",
    "                           metadata=metadata,\n",
    "                           heads=num_heads[i])\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        final_agg_dim = sum(hidden_channels[1:])\n",
    "        self.out_lin = Linear(final_agg_dim, out_channels)\n",
    "\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Initial projection\n",
    "        projected_x_dict = {}\n",
    "        for node_type, x in x_dict.items():\n",
    "            if node_type in self.lin_dict:\n",
    "                projected_x = self.lin_dict[node_type](x)\n",
    "                projected_x = F.gelu(projected_x)\n",
    "                projected_x = self.dropout(projected_x)\n",
    "                projected_x_dict[node_type] = projected_x\n",
    "            elif x is not None:\n",
    "                 projected_x_dict[node_type] = x\n",
    "\n",
    "        layer_outputs = {nt: [] for nt in projected_x_dict.keys()}\n",
    "\n",
    "        # HGT convolutions\n",
    "        current_x_dict = projected_x_dict\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            current_x_dict = conv(current_x_dict, edge_index_dict)\n",
    "            for node_type, x in current_x_dict.items():\n",
    "                 if node_type in layer_outputs:\n",
    "                     layer_outputs[node_type].append(x)\n",
    "\n",
    "        # Concatenate outputs\n",
    "        final_x_dict = {}\n",
    "        for node_type in projected_x_dict.keys():\n",
    "            if node_type in layer_outputs and layer_outputs[node_type]:\n",
    "                try:\n",
    "                    final_x_dict[node_type] = torch.cat(layer_outputs[node_type], dim=1)\n",
    "                except RuntimeError as e:\n",
    "                     print(f\"Error concatenating layer outputs for {node_type}: {e}\")\n",
    "                     for j, tensor in enumerate(layer_outputs[node_type]):\n",
    "                         print(f\"  Layer {j+1} shape: {tensor.shape}\")\n",
    "                     final_x_dict[node_type] = None\n",
    "            else:\n",
    "                final_x_dict[node_type] = None\n",
    "\n",
    "        # Final linear layer and activation\n",
    "        out_dict = {}\n",
    "        for node_type, final_x in final_x_dict.items():\n",
    "             if final_x is not None:\n",
    "                  lin_out = self.out_lin(final_x)\n",
    "                  out_dict[node_type] = F.gelu(lin_out)\n",
    "             else:\n",
    "                  out_dict[node_type] = None\n",
    "\n",
    "        drug_embed = out_dict.get(NODE_TYPE_DRUG, None)\n",
    "        disease_embed = out_dict.get(NODE_TYPE_DISEASE, None)\n",
    "\n",
    "        if drug_embed is None and NODE_TYPE_DRUG in x_dict: pass\n",
    "        if disease_embed is None and NODE_TYPE_DISEASE in x_dict: pass\n",
    "\n",
    "        return drug_embed, disease_embed\n",
    "\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    \"\"\"MLP Predictor identical to hgtdrOG.py.\"\"\"\n",
    "    def __init__(self, channel_num, dropout):\n",
    "        super().__init__()\n",
    "        self.L1 = nn.Linear(channel_num * 2, channel_num)\n",
    "        self.L2 = nn.Linear(channel_num, 1)\n",
    "        self.bn = nn.BatchNorm1d(num_features=channel_num)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, drug_embeddings, disease_embeddings):\n",
    "        x = torch.cat((drug_embeddings, disease_embeddings), dim=1)\n",
    "        x = self.L1(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.dropout(x)\n",
    "        x = self.L2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc7701",
   "metadata": {},
   "source": [
    "## 8. Batching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch, device):\n",
    "    \"\"\"Prepares a training batch with positive/negative samples.\"\"\"\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    indication_edge_type = (NODE_TYPE_DRUG, INDICATION_RELATION, NODE_TYPE_DISEASE)\n",
    "    rev_indication_edge_type = (NODE_TYPE_DISEASE, INDICATION_RELATION, NODE_TYPE_DRUG)\n",
    "\n",
    "    if indication_edge_type not in batch.edge_types or \\\n",
    "    not hasattr(batch[indication_edge_type], 'edge_label_index') or \\\n",
    "    not hasattr(batch[indication_edge_type], 'edge_label'):\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        if 'mask' in batch[indication_edge_type]: del batch[indication_edge_type].mask\n",
    "        return batch\n",
    "\n",
    "    msg_edge_index = batch[indication_edge_type].edge_index\n",
    "    supervision_edge_label_index = batch[indication_edge_type].edge_label_index\n",
    "    supervision_edge_label = batch[indication_edge_type].edge_label\n",
    "\n",
    "    if NODE_TYPE_DRUG not in batch.node_types or NODE_TYPE_DISEASE not in batch.node_types:\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        if 'mask' in batch[indication_edge_type]: del batch[indication_edge_type].mask\n",
    "        return batch\n",
    "\n",
    "    batch_size = batch[NODE_TYPE_DRUG].batch_size\n",
    "    global_to_local_drug = {gid.item(): i for i, gid in enumerate(batch[NODE_TYPE_DRUG].n_id)}\n",
    "    global_to_local_disease = {gid.item(): i for i, gid in enumerate(batch[NODE_TYPE_DISEASE].n_id)}\n",
    "\n",
    "    local_pos_src, local_pos_dst = [], []\n",
    "    for i in range(supervision_edge_label_index.shape[1]):\n",
    "        g_src = supervision_edge_label_index[0, i].item()\n",
    "        g_dst = supervision_edge_label_index[1, i].item()\n",
    "        if g_src in global_to_local_drug and g_dst in global_to_local_disease:\n",
    "            local_src_idx = global_to_local_drug[g_src]\n",
    "            if local_src_idx < batch_size:\n",
    "                local_pos_src.append(local_src_idx)\n",
    "                local_pos_dst.append(global_to_local_disease[g_dst])\n",
    "\n",
    "    if not local_pos_src:\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        if 'mask' in batch[indication_edge_type]: del batch[indication_edge_type].mask\n",
    "        return batch\n",
    "\n",
    "    pos_edge_label_index_local = torch.tensor([local_pos_src, local_pos_dst], dtype=torch.long, device=device)\n",
    "    pos_num = pos_edge_label_index_local.shape[1]\n",
    "    pos_edge_label = torch.ones(pos_num, device=device)\n",
    "\n",
    "    neg_edges_source_local = []\n",
    "    neg_edges_dest_local = []\n",
    "    num_neg_needed = pos_num\n",
    "    attempts = 0\n",
    "    max_attempts = num_neg_needed * 20\n",
    "\n",
    "    # --- Get number of disease nodes ---\n",
    "    num_disease_nodes_in_batch = 0\n",
    "    if NODE_TYPE_DISEASE in batch.node_types:\n",
    "        # Access the number of nodes using the node type as a key into the batch object\n",
    "        # The size of any attribute like 'x' or 'n_id' gives the node count for that type\n",
    "        if hasattr(batch[NODE_TYPE_DISEASE], 'n_id'):\n",
    "            num_disease_nodes_in_batch = batch[NODE_TYPE_DISEASE].n_id.shape[0]\n",
    "        elif hasattr(batch[NODE_TYPE_DISEASE], 'x') and batch[NODE_TYPE_DISEASE].x is not None:\n",
    "            num_disease_nodes_in_batch = batch[NODE_TYPE_DISEASE].x.shape[0]\n",
    "        \n",
    "    if num_disease_nodes_in_batch == 0:\n",
    "        # print(f\"Warning: Could not determine number of disease nodes in batch.\")\n",
    "        pass\n",
    "\n",
    "    while len(neg_edges_source_local) < num_neg_needed and attempts < max_attempts and num_disease_nodes_in_batch > 0:\n",
    "        attempts += 1\n",
    "        source_local = random.randint(0, batch_size - 1)\n",
    "        dest_local = random.randint(0, num_disease_nodes_in_batch - 1)\n",
    "        neg_edge_local = torch.tensor([[source_local], [dest_local]], dtype=torch.long, device=device)\n",
    "\n",
    "        is_in_msg_edges = edge_exists(msg_edge_index, neg_edge_local)\n",
    "        is_in_pos_supervision = edge_exists(pos_edge_label_index_local, neg_edge_local)\n",
    "\n",
    "        if not is_in_msg_edges and not is_in_pos_supervision:\n",
    "            neg_edges_source_local.append(source_local)\n",
    "            neg_edges_dest_local.append(dest_local)\n",
    "\n",
    "    if len(neg_edges_source_local) < num_neg_needed:\n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"Warning: Could only sample {len(neg_edges_source_local)}/{num_neg_needed} neg edges after {max_attempts} attempts.\")\n",
    "\n",
    "    neg_edge_label_index_local = torch.tensor([neg_edges_source_local, neg_edges_dest_local], dtype=torch.long, device=device)\n",
    "    neg_edge_label = torch.zeros(len(neg_edges_source_local), device=device)\n",
    "\n",
    "    final_edge_label_index = torch.cat((pos_edge_label_index_local, neg_edge_label_index_local), dim=1)\n",
    "    final_edge_label = torch.cat((pos_edge_label, neg_edge_label), dim=0)\n",
    "\n",
    "    batch[indication_edge_type].edge_label_index = final_edge_label_index\n",
    "    batch[indication_edge_type].edge_label = final_edge_label\n",
    "\n",
    "    if 'mask' in batch[indication_edge_type]:\n",
    "        del batch[indication_edge_type].mask\n",
    "    if rev_indication_edge_type in batch.edge_types and 'mask' in batch[rev_indication_edge_type]:\n",
    "        del batch[rev_indication_edge_type].mask\n",
    "\n",
    "    return batch\n",
    "\n",
    "def make_test_batch(batch, full_data, device):\n",
    "    \"\"\"Prepares a validation/test batch with positive/negative samples.\"\"\"\n",
    "    batch = batch.to(device)\n",
    "    indication_edge_type = (NODE_TYPE_DRUG, INDICATION_RELATION, NODE_TYPE_DISEASE)\n",
    "\n",
    "    if indication_edge_type not in batch.edge_types or \\\n",
    "    not hasattr(batch[indication_edge_type], 'edge_label_index') or \\\n",
    "    not hasattr(batch[indication_edge_type], 'edge_label'):\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        return batch\n",
    "\n",
    "    if NODE_TYPE_DRUG not in batch.node_types or NODE_TYPE_DISEASE not in batch.node_types:\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        return batch\n",
    "\n",
    "\n",
    "    batch_size = batch[NODE_TYPE_DRUG].batch_size\n",
    "    pos_edge_label_index_global = batch[indication_edge_type].edge_label_index\n",
    "    pos_edge_label_global = batch[indication_edge_type].edge_label\n",
    "\n",
    "    global_to_local_drug = {gid.item(): i for i, gid in enumerate(batch[NODE_TYPE_DRUG].n_id)}\n",
    "    global_to_local_disease = {gid.item(): i for i, gid in enumerate(batch[NODE_TYPE_DISEASE].n_id)}\n",
    "\n",
    "    local_pos_src, local_pos_dst, kept_labels = [], [], []\n",
    "    for i in range(pos_edge_label_index_global.shape[1]):\n",
    "        g_src = pos_edge_label_index_global[0, i].item()\n",
    "        g_dst = pos_edge_label_index_global[1, i].item()\n",
    "        if g_src in global_to_local_drug and g_dst in global_to_local_disease:\n",
    "            local_src_idx = global_to_local_drug[g_src]\n",
    "            if local_src_idx < batch_size:\n",
    "                local_pos_src.append(local_src_idx)\n",
    "                local_pos_dst.append(global_to_local_disease[g_dst])\n",
    "                kept_labels.append(pos_edge_label_global[i])\n",
    "\n",
    "    if not local_pos_src:\n",
    "        batch[indication_edge_type].edge_label_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        batch[indication_edge_type].edge_label = torch.empty(0, dtype=torch.float, device=device)\n",
    "        return batch\n",
    "\n",
    "    pos_edge_label_index_local = torch.tensor([local_pos_src, local_pos_dst], dtype=torch.long, device=device)\n",
    "    pos_edge_label_local = torch.tensor(kept_labels, dtype=torch.float, device=device)\n",
    "    pos_num = pos_edge_label_index_local.shape[1]\n",
    "\n",
    "    neg_edges_source_local = []\n",
    "    neg_edges_dest_local = []\n",
    "    num_neg_needed = pos_num\n",
    "    attempts = 0\n",
    "    max_attempts = num_neg_needed * 20\n",
    "\n",
    "    original_indication_edges = full_data[indication_edge_type].edge_index.to(device)\n",
    "\n",
    "    # --- Get number of disease nodes (Eval) ---\n",
    "    num_disease_nodes_in_batch = 0\n",
    "    if NODE_TYPE_DISEASE in batch.node_types:\n",
    "        if hasattr(batch[NODE_TYPE_DISEASE], 'n_id'):\n",
    "            num_disease_nodes_in_batch = batch[NODE_TYPE_DISEASE].n_id.shape[0]\n",
    "        elif hasattr(batch[NODE_TYPE_DISEASE], 'x') and batch[NODE_TYPE_DISEASE].x is not None:\n",
    "            num_disease_nodes_in_batch = batch[NODE_TYPE_DISEASE].x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    batch_global_drug_ids = batch[NODE_TYPE_DRUG].n_id\n",
    "    batch_global_disease_ids = batch[NODE_TYPE_DISEASE].n_id if num_disease_nodes_in_batch > 0 else torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "    while len(neg_edges_source_local) < num_neg_needed and attempts < max_attempts and num_disease_nodes_in_batch > 0:\n",
    "        attempts += 1\n",
    "        source_local = random.randint(0, batch_size - 1)\n",
    "        dest_local = random.randint(0, num_disease_nodes_in_batch - 1)\n",
    "        global_drug_id = batch_global_drug_ids[source_local]\n",
    "        global_disease_id = batch_global_disease_ids[dest_local]\n",
    "        neg_edge_global = torch.tensor([[global_drug_id], [global_disease_id]], dtype=torch.long, device=device)\n",
    "        if not edge_exists(original_indication_edges, neg_edge_global):\n",
    "            neg_edges_source_local.append(source_local)\n",
    "            neg_edges_dest_local.append(dest_local)\n",
    "\n",
    "    if len(neg_edges_source_local) < num_neg_needed:\n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"Warning: Test/Val could only sample {len(neg_edges_source_local)}/{num_neg_needed} neg edges after {max_attempts} attempts.\")\n",
    "\n",
    "    neg_edge_label_index_local = torch.tensor([neg_edges_source_local, neg_edges_dest_local], dtype=torch.long, device=device)\n",
    "    neg_edge_label_local = torch.zeros(len(neg_edges_source_local), device=device)\n",
    "\n",
    "    final_edge_label_index = torch.cat((pos_edge_label_index_local, neg_edge_label_index_local), dim=1)\n",
    "    final_edge_label = torch.cat((pos_edge_label_local, neg_edge_label_local), dim=0)\n",
    "\n",
    "    batch[indication_edge_type].edge_label_index = final_edge_label_index\n",
    "    batch[indication_edge_type].edge_label = final_edge_label\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32607765",
   "metadata": {},
   "source": [
    "## 9. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(gnn, predictor, loader, optimizer, device):\n",
    "    \"\"\"Runs a single training epoch.\"\"\"\n",
    "    gnn.train()\n",
    "    predictor.train()\n",
    "     total_examples = total_loss = 0.0\n",
    "\n",
    "     for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Prepare batch (masking, negative sampling) using the function\n",
    "        processed_batch = make_batch(copy.deepcopy(batch), device) # Use deepcopy to avoid modifying loader's cache\n",
    "\n",
    "        # Extract labels and indices for loss *after* make_batch processing\n",
    "        edge_label_index = processed_batch[INDICATION_EDGE_TYPE].edge_label_index\n",
    "        edge_label = processed_batch[INDICATION_EDGE_TYPE].edge_label\n",
    "\n",
    "        if edge_label.shape[0] == 0: # Skip if batch ended up with no pos/neg samples\n",
    "            continue\n",
    "\n",
    "        # Run GNN forward pass on the processed batch (contains message passing edges)\n",
    "        drug_embed, disease_embed = gnn(processed_batch.x_dict, processed_batch.edge_index_dict)\n",
    "\n",
    "        # Check if embeddings were generated\n",
    "        if drug_embed is None or disease_embed is None:\n",
    "             print(\"Warning: Skipping batch due to missing drug/disease embeddings.\")\n",
    "             continue\n",
    "        if drug_embed.shape[0] == 0 or disease_embed.shape[0] == 0:\n",
    "             print(\"Warning: Skipping batch due to empty drug/disease embeddings.\")\n",
    "             continue\n",
    "\n",
    "        # Select embeddings for loss calculation using edge_label_index\n",
    "        # Ensure indices are within bounds\n",
    "        if edge_label_index.numel() > 0: # Check if there are edges to predict\n",
    "             max_drug_idx = edge_label_index[0].max().item() if edge_label_index[0].numel() > 0 else -1\n",
    "             max_disease_idx = edge_label_index[1].max().item() if edge_label_index[1].numel() > 0 else -1\n",
    "\n",
    "             if max_drug_idx >= drug_embed.shape[0] or max_disease_idx >= disease_embed.shape[0]:\n",
    "                 print(f\"Warning: Index out of bounds. Max Drug Idx: {max_drug_idx} (Embed shape: {drug_embed.shape}), Max Disease Idx: {max_disease_idx} (Embed shape: {disease_embed.shape}). Skipping batch.\")\n",
    "                 # print(\"Problematic edge_label_index:\", edge_label_index)\n",
    "                 # print(\"Batch drug n_ids:\", processed_batch[NODE_TYPE_DRUG].n_id)\n",
    "                 # print(\"Batch disease n_ids:\", processed_batch[NODE_TYPE_DISEASE].n_id if NODE_TYPE_DISEASE in processed_batch.node_types else \"N/A\")\n",
    "                 continue\n",
    "\n",
    "             drug_nodes_for_pred = drug_embed[edge_label_index[0]]\n",
    "             disease_nodes_for_pred = disease_embed[edge_label_index[1]]\n",
    "        else:\n",
    "             # print(\"Warning: No edges found in edge_label_index for loss calculation.\")\n",
    "             continue # Skip if no edges to calculate loss on\n",
    "\n",
    "\n",
    "        # Get predictions\n",
    "        out = predictor(drug_nodes_for_pred, disease_nodes_for_pred).squeeze(-1) # Remove channel dim\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = compute_loss(out, edge_label)\n",
    "\n",
    "        # Backpropagate\n",
    "        if torch.isnan(loss):\n",
    "             print(\"Warning: NaN loss detected. Skipping backward pass for this batch.\")\n",
    "             # Optionally add more debugging here (print inputs, model weights, etc.)\n",
    "        elif loss.requires_grad:\n",
    "             loss.backward()\n",
    "             # Optional: Gradient Clipping\n",
    "             # torch.nn.utils.clip_grad_norm_(list(gnn.parameters()) + list(predictor.parameters()), max_norm=1.0)\n",
    "             optimizer.step()\n",
    "             # Update totals\n",
    "             current_batch_examples = edge_label_index.shape[1]\n",
    "             total_examples += current_batch_examples\n",
    "             total_loss += float(loss) * current_batch_examples # Weighted average\n",
    "        else:\n",
    "             print(\"Warning: Loss does not require grad. Skipping backward/step.\")\n",
    "\n",
    "\n",
    "    return total_loss / total_examples if total_examples > 0 else 0.0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(gnn, predictor, loader, full_data, device):\n",
    "    \"\"\"Evaluates the model on a validation or test loader.\"\"\"\n",
    "    gnn.eval()\n",
    "    predictor.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        # Prepare batch for evaluation (finds relevant true positives, samples negatives)\n",
    "        processed_batch = make_test_batch(copy.deepcopy(batch), full_data, device)\n",
    "\n",
    "        # Extract labels and indices for evaluation *after* make_test_batch processing\n",
    "        edge_label_index = processed_batch[INDICATION_EDGE_TYPE].edge_label_index\n",
    "        edge_label = processed_batch[INDICATION_EDGE_TYPE].edge_label\n",
    "\n",
    "        if edge_label.shape[0] == 0: # Skip if no evaluation samples in batch\n",
    "            continue\n",
    "\n",
    "        # Run GNN forward pass on the original batch structure (full message passing)\n",
    "        # Note: make_test_batch doesn't modify edge_index used for message passing\n",
    "        drug_embed, disease_embed = gnn(processed_batch.x_dict, processed_batch.edge_index_dict)\n",
    "\n",
    "        if drug_embed is None or disease_embed is None: continue\n",
    "        if drug_embed.shape[0] == 0 or disease_embed.shape[0] == 0: continue\n",
    "\n",
    "\n",
    "        # Select embeddings using the edge_label_index prepared by make_test_batch\n",
    "        if edge_label_index.numel() > 0:\n",
    "             max_drug_idx = edge_label_index[0].max().item() if edge_label_index[0].numel() > 0 else -1\n",
    "             max_disease_idx = edge_label_index[1].max().item() if edge_label_index[1].numel() > 0 else -1\n",
    "\n",
    "             if max_drug_idx >= drug_embed.shape[0] or max_disease_idx >= disease_embed.shape[0]:\n",
    "                 print(f\"Warning (Eval): Index out of bounds. Max Drug Idx: {max_drug_idx}, Max Disease Idx: {max_disease_idx}. Skipping batch.\")\n",
    "                 continue\n",
    "\n",
    "             drug_nodes_for_pred = drug_embed[edge_label_index[0]]\n",
    "             disease_nodes_for_pred = disease_embed[edge_label_index[1]]\n",
    "        else:\n",
    "             continue # Skip if no edges\n",
    "\n",
    "\n",
    "        # Get predictions\n",
    "        out = predictor(drug_nodes_for_pred, disease_nodes_for_pred).squeeze(-1)\n",
    "\n",
    "        all_preds.append(out)\n",
    "        all_labels.append(edge_label)\n",
    "\n",
    "    if not all_preds:\n",
    "        print(\"Warning: No predictions made during evaluation.\")\n",
    "        return torch.tensor(0.0), torch.tensor([]), torch.tensor([]) # loss, preds, labels\n",
    "\n",
    "    all_preds_tensor = torch.cat(all_preds)\n",
    "    all_labels_tensor = torch.cat(all_labels)\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = compute_loss(all_preds_tensor, all_labels_tensor)\n",
    "\n",
    "    return val_loss, all_preds_tensor, all_labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f73dd",
   "metadata": {},
   "source": [
    "## 10. Main Cross-Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fold_results = [] # Store metrics for each fold\n",
    "\n",
    "print(\"\\n=== Starting 5-Fold Cross-Validation ===\")\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    fold_start_time = time.time()\n",
    "    device = get_device_for_fold(fold_num)\n",
    "    write_to_out(f\"\\n--- Starting Fold {fold_num}/{5} on device {device} ---\", fold_num=fold_num)\n",
    "\n",
    "    # 1. Preprocess data for the current fold\n",
    "\n",
    "    train_data, val_data, full_data, entity_dictionary, node_feature_dims = preprocess_fold_data(\n",
    "        fold_num,\n",
    "        df_kg=df_primekg_raw,\n",
    "        biobert_df=biobert_embeddings_df,\n",
    "        chemberta_df=chemberta_embeddings_df,\n",
    "        transe_npy=transe_embeddings_npy,\n",
    "        pykeen_map=pykeen_entity_to_id\n",
    "        # inference_mode=False is default\n",
    "    )\n",
    "\n",
    "    if train_data is None: # Check if preprocessing failed\n",
    "        write_to_out(f\"Error: Preprocessing failed for Fold {fold_num}. Skipping.\", fold_num=fold_num)\n",
    "        continue\n",
    "\n",
    "    # 2. Define Model and Optimizer\n",
    "    try:\n",
    "        gnn = HGT_Combined(\n",
    "            hidden_channels=HGTDR_CONFIG['hidden_channels'],\n",
    "            out_channels=HGTDR_CONFIG['out_channels'],\n",
    "            num_heads=HGTDR_CONFIG['num_heads'],\n",
    "            num_layers=HGTDR_CONFIG['num_layers'],\n",
    "            dropout=HGTDR_CONFIG['dropout'],\n",
    "            node_feature_dims=node_feature_dims,\n",
    "            metadata=train_data.metadata() # Get metadata from the created train_data\n",
    "        ).to(device)\n",
    "\n",
    "        predictor = MLPPredictor(\n",
    "            channel_num=HGTDR_CONFIG['out_channels'],\n",
    "            dropout=HGTDR_CONFIG['predictor_dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        parameters = list(gnn.parameters()) + list(predictor.parameters())\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            parameters,\n",
    "            lr=HGTDR_CONFIG['learning_rate'],\n",
    "            weight_decay=HGTDR_CONFIG['weight_decay']\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=HGTDR_CONFIG['epochs'],\n",
    "            eta_min=0 # Cosine annealing to 0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        write_to_out(f\"Error initializing model/optimizer for Fold {fold_num}: {e}\", fold_num=fold_num)\n",
    "        # Clean up partial data if needed\n",
    "        del train_data, val_data, full_data, entity_dictionary, node_feature_dims\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        continue # Skip fold\n",
    "\n",
    "    # 3. Define Data Loaders\n",
    "    primary_input_node = NODE_TYPE_DRUG if NODE_TYPE_DRUG in train_data.node_types else list(train_data.node_types)[0]\n",
    "    print(f\"Using '{primary_input_node}' as primary input node for HGTLoader.\")\n",
    "    loader_kwargs = {'batch_size': HGTDR_CONFIG['batch_size'], 'num_workers': 2, 'persistent_workers': False, 'pin_memory': True if device != torch.device('cpu') else False}\n",
    "\n",
    "    try:\n",
    "        if primary_input_node not in train_data.node_types:\n",
    "            raise ValueError(f\"Primary input node '{primary_input_node}' not found in train_data for Fold {fold_num}.\")\n",
    "\n",
    "        train_loader = HGTLoader(\n",
    "            train_data,\n",
    "            num_samples=[HGTDR_CONFIG['num_samples']] * HGTDR_CONFIG['num_layers'],\n",
    "            shuffle=True,\n",
    "            input_nodes=(primary_input_node, None),\n",
    "            **loader_kwargs\n",
    "        )\n",
    "        val_loader = HGTLoader(\n",
    "            val_data, # Use val_data for validation loader\n",
    "            num_samples=[HGTDR_CONFIG['num_samples']] * HGTDR_CONFIG['num_layers'],\n",
    "            shuffle=False, # No shuffle for validation\n",
    "            input_nodes=(primary_input_node, None),\n",
    "            **loader_kwargs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        write_to_out(f\"Error creating loaders for Fold {fold_num}: {e}\", fold_num=fold_num)\n",
    "        del gnn, predictor, optimizer, scheduler # Clean up model parts\n",
    "        del train_data, val_data, full_data, entity_dictionary, node_feature_dims\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # 4. Training Loop with Checkpointing\n",
    "    train_losses, val_losses = [], []\n",
    "    # --- Checkpointing Variables ---\n",
    "    best_val_aupr_fold = -1.0 # Initialize best AUPR for this fold\n",
    "    best_gnn_state_dict_fold = None\n",
    "    best_predictor_state_dict_fold = None\n",
    "    best_epoch_fold = -1\n",
    "    # --- End Checkpointing Variables ---\n",
    "\n",
    "    write_to_out(f\"Starting training for Fold {fold_num} ({HGTDR_CONFIG['epochs']} epochs)...\", fold_num=fold_num)\n",
    "    for epoch in range(HGTDR_CONFIG['epochs']):\n",
    "        epoch_start_time = time.time()\n",
    "        loss = train_epoch(gnn, predictor, train_loader, optimizer, device)\n",
    "\n",
    "        # Evaluate validation performance periodically for checkpointing and logging\n",
    "        current_val_loss = float('nan')\n",
    "        current_val_aupr = float('nan')\n",
    "        current_val_auroc = float('nan')\n",
    "        # Evaluate more frequently early on, less frequently later\n",
    "        eval_freq = 5 if epoch < 20 else 10 if epoch < 100 else 25\n",
    "        should_evaluate = (epoch + 1) % eval_freq == 0 or epoch == HGTDR_CONFIG['epochs'] - 1\n",
    "\n",
    "        if should_evaluate:\n",
    "            val_loss_tensor, val_preds, val_labels = evaluate(gnn, predictor, val_loader, full_data, device)\n",
    "            current_val_loss = val_loss_tensor.item()\n",
    "\n",
    "            if val_labels.numel() > 0: # Check if evaluation produced results\n",
    "                current_val_auroc, current_val_aupr = calculate_metrics(val_preds, val_labels, fold_num) # Capture metrics\n",
    "            else:\n",
    "                current_val_auroc, current_val_aupr = 0.0, 0.0 \n",
    "                write_to_out(f\"Fold {fold_num} Epoch {epoch+1}: No validation samples processed.\", fold_num=fold_num)\n",
    "\n",
    "            val_losses.append(current_val_loss) # Append only when evaluated\n",
    "            train_losses.append(loss) # Append corresponding train loss\n",
    "\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            lr = optimizer.param_groups[0]['lr'] # Get current learning rate\n",
    "            # Log validation AUPR along with loss\n",
    "            write_to_out(f'F{fold_num} E: {epoch+1:03d}, TrL: {loss:.4f}, VaL: {current_val_loss:.4f}, VaAUPR: {current_val_aupr:.4f}, LR: {lr:.6f}, T: {epoch_duration:.2f}s', fold_num=fold_num)\n",
    "\n",
    "            # --- Checkpointing Logic (based on AUPR) ---\n",
    "            if current_val_aupr > best_val_aupr_fold:\n",
    "                best_val_aupr_fold = current_val_aupr\n",
    "                best_epoch_fold = epoch + 1\n",
    "                best_gnn_state_dict_fold = copy.deepcopy(gnn.state_dict())\n",
    "                best_predictor_state_dict_fold = copy.deepcopy(predictor.state_dict())\n",
    "                print(f\"    ** New best AUPR for Fold {fold_num}: {best_val_aupr_fold:.4f} at epoch {best_epoch_fold} **\")\n",
    "            # --- End Checkpointing ---\n",
    "\n",
    "            # Save intermediate loss plot\n",
    "            if train_losses and val_losses:\n",
    "                plot_losses(train_losses, val_losses, filename=f\"losses_epoch_{epoch+1}.png\", fold_num=fold_num)\n",
    "\n",
    "        else:\n",
    "            # Optional: Print train loss more often if desired, even without validation\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                write_to_out(f'F{fold_num} E: {epoch+1:03d}, TrL: {loss:.4f}, T: {epoch_duration:.2f}s', fold_num=fold_num)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "    # 5. Final Evaluation & Reporting for this Fold\n",
    "    write_to_out(f\"--- Evaluating Fold {fold_num} with FINAL model state ---\", fold_num=fold_num)\n",
    "    final_val_loss, final_val_preds, final_val_labels = evaluate(gnn, predictor, val_loader, full_data, device)\n",
    "\n",
    "    if final_val_labels.numel() > 0:\n",
    "        final_auroc, final_aupr = calculate_metrics(final_val_preds, final_val_labels, fold_num)\n",
    "        # Store results - maybe store both final and best? Adjust as needed.\n",
    "        # Storing final metrics here for consistency with potential original paper reporting style\n",
    "        all_fold_results.append({\n",
    "            'fold': fold_num,\n",
    "            'final_auroc': final_auroc,\n",
    "            'final_aupr': final_aupr,\n",
    "            'final_loss': final_val_loss.item(),\n",
    "            'best_val_aupr': best_val_aupr_fold, # Also record the best achieved AUPR\n",
    "            'best_epoch': best_epoch_fold\n",
    "        })\n",
    "    else:\n",
    "        write_to_out(f\"Fold {fold_num}: No validation samples processed for final evaluation.\", fold_num=fold_num)\n",
    "        all_fold_results.append({\n",
    "            'fold': fold_num, 'final_auroc': 0.0, 'final_aupr': 0.0, 'final_loss': final_val_loss.item(),\n",
    "            'best_val_aupr': best_val_aupr_fold, 'best_epoch': best_epoch_fold\n",
    "        })\n",
    "\n",
    "\n",
    "    # 6. Save the BEST Model State for this Fold\n",
    "    if best_gnn_state_dict_fold and best_predictor_state_dict_fold:\n",
    "        gnn_save_path = os.path.join(OUT_DIR, f'Fold_{fold_num}_best_gnn.pth')\n",
    "        predictor_save_path = os.path.join(OUT_DIR, f'Fold_{fold_num}_best_predictor.pth')\n",
    "        try:\n",
    "            torch.save(best_gnn_state_dict_fold, gnn_save_path)\n",
    "            torch.save(best_predictor_state_dict_fold, predictor_save_path)\n",
    "            write_to_out(f\"Saved BEST model for Fold {fold_num} (Epoch: {best_epoch_fold}, AUPR: {best_val_aupr_fold:.4f}) to {OUT_DIR}\", fold_num=fold_num)\n",
    "        except Exception as e:\n",
    "            write_to_out(f\"Error saving best model for Fold {fold_num}: {e}\", fold_num=fold_num)\n",
    "    else:\n",
    "        write_to_out(f\"Warning: No best model state was saved for Fold {fold_num} (perhaps no improvement).\", fold_num=fold_num)\n",
    "\n",
    "    # 7. Plot final losses for this fold\n",
    "    if train_losses and val_losses:\n",
    "         plot_losses(train_losses, val_losses, filename=\"final_losses.png\", fold_num=fold_num)\n",
    "\n",
    "    fold_duration = time.time() - fold_start_time\n",
    "    write_to_out(f\"--- Fold {fold_num} Finished in {fold_duration:.2f} seconds ---\", fold_num=fold_num)\n",
    "\n",
    "    # 8. Clean up memory\n",
    "    del gnn, predictor, optimizer, scheduler, train_loader, val_loader\n",
    "    # Also delete saved best states if memory is very tight, though usually not necessary\n",
    "    del best_gnn_state_dict_fold, best_predictor_state_dict_fold\n",
    "    del train_data, val_data, full_data, entity_dictionary, node_feature_dims # Remove fold-specific data\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        # print(f\"Cleared CUDA cache for device {device}\")\n",
    "\n",
    "print(\"\\n=== Cross-Validation Finished ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54832b4c",
   "metadata": {},
   "source": [
    "## 11. Aggregate and Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450267ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_fold_results:\n",
    "    results_df = pd.DataFrame(all_fold_results)\n",
    "    print(\"\\n--- Aggregated 5-Fold CV Results ---\")\n",
    "    # Use round() for cleaner display\n",
    "    print(results_df.round(4).to_markdown(index=False))\n",
    "\n",
    "    # --- Use the correct column names ---\n",
    "    mean_auroc = results_df['final_auroc'].mean() # Changed from 'auroc'\n",
    "    std_auroc = results_df['final_auroc'].std()   # Changed from 'auroc'\n",
    "    mean_aupr = results_df['final_aupr'].mean()   # Changed from 'aupr'\n",
    "    std_aupr = results_df['final_aupr'].std()     # Changed from 'aupr'\n",
    "    # --- End Correction ---\n",
    "\n",
    "    # Also report mean best AUPR achieved during training\n",
    "    mean_best_aupr = results_df['best_val_aupr'].mean()\n",
    "    std_best_aupr = results_df['best_val_aupr'].std()\n",
    "\n",
    "\n",
    "    summary = f\"\"\"\n",
    "    -----------------------------------------\n",
    "    Cross-Validation Summary (HGTDR + TransE):\n",
    "\n",
    "    Final Model Performance (End of Training):\n",
    "      Mean AUROC: {mean_auroc:.4f} +/- {std_auroc:.4f}\n",
    "      Mean AUPR:  {mean_aupr:.4f} +/- {std_aupr:.4f}\n",
    "\n",
    "    Best Validation Performance (During Training):\n",
    "      Mean Best Val AUPR: {mean_best_aupr:.4f} +/- {std_best_aupr:.4f}\n",
    "      (Based on checkpoints saved during training)\n",
    "    -----------------------------------------\n",
    "    \"\"\"\n",
    "    print(summary)\n",
    "    # Write summary to a general output file\n",
    "    write_to_out(summary, filename=\"summary_results.txt\")\n",
    "\n",
    "    # Save detailed results to CSV\n",
    "    results_save_path = os.path.join(OUT_DIR, \"detailed_results.csv\")\n",
    "    try:\n",
    "        # Save with rounding for consistency\n",
    "        results_df.round(4).to_csv(results_save_path, index=False)\n",
    "        print(f\"Saved detailed results to: {results_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving detailed results CSV: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No results generated. Please check for errors in the cross-validation loop.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\") # Move this if Cell 12 follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e8686",
   "metadata": {},
   "source": [
    "## 11.5. Prepare Full Data for Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c772908",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing Full Data for Inference ---\")\n",
    "try:\n",
    "    # Call preprocess_fold_data with inference_mode=True\n",
    "    _, _, full_data_for_inference, entity_dictionary_for_inference, _ = preprocess_fold_data(\n",
    "        fold_num=0, # Placeholder fold number\n",
    "        df_kg=df_primekg_raw,\n",
    "        biobert_df=biobert_embeddings_df,\n",
    "        chemberta_df=chemberta_embeddings_df,\n",
    "        transe_npy=transe_embeddings_npy,\n",
    "        pykeen_map=pykeen_entity_to_id,\n",
    "        inference_mode=True  # <-- Tell the function to skip split loading/masking\n",
    "    )\n",
    "\n",
    "    if full_data_for_inference is None or entity_dictionary_for_inference is None:\n",
    "         raise RuntimeError(\"Preprocessing failed to return necessary data in inference mode.\")\n",
    "\n",
    "    print(\"Full data prepared successfully for inference.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: Required dataframes/mappings not found. Ensure previous cells were run. ({e})\")\n",
    "    full_data_for_inference = None\n",
    "    entity_dictionary_for_inference = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final preprocessing: {e}\")\n",
    "    full_data_for_inference = None\n",
    "    entity_dictionary_for_inference = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9800d64",
   "metadata": {},
   "source": [
    "## 12. Generate Drug Repurposing Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Generating Drug Repurposing Candidates ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_CANDIDATES_TO_SHOW = 50\n",
    "INFERENCE_BATCH_SIZE = 512\n",
    "FOLD_TO_LOAD = 5 # <<< CHOOSE WHICH FOLD'S BEST MODEL TO USE (e.g., 5 or the overall best)\n",
    "\n",
    "# --- Use the prepared data ---\n",
    "if 'full_data_for_inference' not in locals() or full_data_for_inference is None or \\\n",
    "'entity_dictionary_for_inference' not in locals() or entity_dictionary_for_inference is None:\n",
    "    print(\"Error: Full data/entity dictionary not prepared. Please run the 'Prepare Full Data' cell first.\")\n",
    "    # exit()\n",
    "else:\n",
    "    print(\"Using pre-prepared full_data and entity_dictionary for inference.\")\n",
    "    full_data = full_data_for_inference\n",
    "    entity_dictionary = entity_dictionary_for_inference\n",
    "\n",
    "    # --- Instantiate Model Architecture (needed before loading state_dict) ---\n",
    "    # Ensure node_feature_dims is available or recalculate from full_data\n",
    "    try:\n",
    "        if 'node_feature_dims' not in locals():\n",
    "            print(\"Recalculating node_feature_dims...\")\n",
    "            node_feature_dims = {}\n",
    "            for node_type in full_data.node_types:\n",
    "                if hasattr(full_data[node_type], 'x') and full_data[node_type].x is not None:\n",
    "                    node_feature_dims[node_type] = full_data[node_type].x.shape[1]\n",
    "                else: node_feature_dims[node_type] = 0\n",
    "\n",
    "        # Use a placeholder device first, will be moved later\n",
    "        temp_device = torch.device('cpu')\n",
    "        gnn = HGT_Combined(\n",
    "            hidden_channels=HGTDR_CONFIG['hidden_channels'], out_channels=HGTDR_CONFIG['out_channels'],\n",
    "            num_heads=HGTDR_CONFIG['num_heads'], num_layers=HGTDR_CONFIG['num_layers'],\n",
    "            dropout=HGTDR_CONFIG['dropout'], node_feature_dims=node_feature_dims,\n",
    "            metadata=full_data.metadata()\n",
    "        ).to(temp_device) # Instantiate on CPU first\n",
    "\n",
    "        predictor = MLPPredictor(\n",
    "            channel_num=HGTDR_CONFIG['out_channels'], dropout=HGTDR_CONFIG['predictor_dropout']\n",
    "        ).to(temp_device)\n",
    "    except NameError as e:\n",
    "        print(f\"Error: HGTDR_CONFIG or other necessary components not defined. {e}\")\n",
    "        # exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error instantiating model architecture: {e}\")\n",
    "        # exit()\n",
    "\n",
    "\n",
    "    # --- Load the BEST model state dict ---\n",
    "    gnn_load_path = os.path.join(OUT_DIR, f'Fold_{FOLD_TO_LOAD}_best_gnn.pth')\n",
    "    predictor_load_path = os.path.join(OUT_DIR, f'Fold_{FOLD_TO_LOAD}_best_predictor.pth')\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading best model state from Fold {FOLD_TO_LOAD}...\")\n",
    "        # Load state dict using map_location='cpu' initially in case saved on GPU\n",
    "        gnn.load_state_dict(torch.load(gnn_load_path, map_location='cpu'))\n",
    "        predictor.load_state_dict(torch.load(predictor_load_path, map_location='cpu'))\n",
    "        print(\"Model state loaded successfully.\")\n",
    "\n",
    "        # --- Determine Inference Device and Move Model ---\n",
    "        # Use a GPU if available, otherwise CPU\n",
    "        model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        gnn.to(model_device)\n",
    "        predictor.to(model_device)\n",
    "        gnn.eval()\n",
    "        predictor.eval()\n",
    "        print(f\"Using model on device: {model_device}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model state dict files not found for Fold {FOLD_TO_LOAD}.\")\n",
    "        print(f\"Looked for: {gnn_load_path} and {predictor_load_path}\")\n",
    "        # exit() # Stop if model can't be loaded\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state dict: {e}\")\n",
    "\n",
    "    # --- Get all Drug and Disease Nodes ---\n",
    "    try:\n",
    "        drug_name_to_hgt_id = entity_dictionary.get(NODE_TYPE_DRUG, {}); disease_name_to_hgt_id = entity_dictionary.get(NODE_TYPE_DISEASE, {})\n",
    "        if not drug_name_to_hgt_id or not disease_name_to_hgt_id: raise ValueError(\"Entity dictionary missing types.\")\n",
    "        all_drug_hgt_ids = list(drug_name_to_hgt_id.values()); all_disease_hgt_ids = list(disease_name_to_hgt_id.values())\n",
    "        hgt_id_to_drug_name = {v: k for k, v in drug_name_to_hgt_id.items()}; hgt_id_to_disease_name = {v: k for k, v in disease_name_to_hgt_id.items()}\n",
    "        num_drugs = len(all_drug_hgt_ids); num_diseases = len(all_disease_hgt_ids); print(f\"Found {num_drugs} drugs and {num_diseases} diseases.\")\n",
    "        known_indications_hgt_ids = set()\n",
    "        if INDICATION_EDGE_TYPE in full_data.edge_types:\n",
    "            known_edges = full_data[INDICATION_EDGE_TYPE].edge_index.t().tolist()\n",
    "            for src_hgt_id, dst_hgt_id in known_edges: known_indications_hgt_ids.add((src_hgt_id, dst_hgt_id))\n",
    "        print(f\"Found {len(known_indications_hgt_ids)} known indications.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting node lists or known indications: {e}\"); predicted_scores, predicted_pairs_hgt_ids = None, None\n",
    "\n",
    "    # --- Perform Inference ---\n",
    "    @torch.no_grad()\n",
    "    def predict_scores(drug_ids, disease_ids):\n",
    "        try:\n",
    "            print(\"Generating node embeddings for full graph...\")\n",
    "            current_full_data = full_data.to(model_device) # Move data to model device\n",
    "            drug_embed_all, disease_embed_all = gnn(current_full_data.x_dict, current_full_data.edge_index_dict)\n",
    "            print(\"Node embeddings generated.\")\n",
    "        except Exception as e: print(f\"Error generating full graph embeddings: {e}.\"); return None, None\n",
    "        if drug_embed_all is None or disease_embed_all is None: print(\"Error: Failed to get embeddings.\"); return None, None\n",
    "        scores = []; pairs = []\n",
    "        drug_ids_tensor = torch.tensor(drug_ids, dtype=torch.long, device=model_device); disease_ids_tensor = torch.tensor(disease_ids, dtype=torch.long, device=model_device)\n",
    "        num_pairs = len(drug_ids)\n",
    "        for i in range(0, num_pairs, INFERENCE_BATCH_SIZE):\n",
    "            batch_drug_ids = drug_ids_tensor[i : i + INFERENCE_BATCH_SIZE]; batch_disease_ids = disease_ids_tensor[i : i + INFERENCE_BATCH_SIZE]\n",
    "            # Handle potential out-of-bounds if node IDs somehow exceed embedding tensor size\n",
    "            if batch_drug_ids.max() >= drug_embed_all.shape[0] or batch_disease_ids.max() >= disease_embed_all.shape[0]:\n",
    "                print(f\"Warning: Skipping inference batch {i//INFERENCE_BATCH_SIZE} due to index out of bounds.\")\n",
    "                continue\n",
    "            batch_drug_embeds = drug_embed_all[batch_drug_ids]; batch_disease_embeds = disease_embed_all[batch_disease_ids]\n",
    "            batch_scores = predictor(batch_drug_embeds, batch_disease_embeds).squeeze(-1)\n",
    "            scores.append(batch_scores.cpu())\n",
    "            for j in range(len(batch_drug_ids)): pairs.append((drug_ids[i+j], disease_ids[i+j]))\n",
    "            if (i // INFERENCE_BATCH_SIZE) % 100 == 0: print(f\"  Processed {i + len(batch_drug_ids)} / {num_pairs} pairs...\")\n",
    "        if not scores: return None, None\n",
    "        all_scores = torch.cat(scores); return all_scores, pairs\n",
    "\n",
    "    # --- Generate Candidate Pairs and Predict ---\n",
    "    if 'all_drug_hgt_ids' in locals():\n",
    "        candidate_drug_ids = []; candidate_disease_ids = []; print(\"Generating candidate drug-disease pairs (excluding known indications)...\"); count = 0\n",
    "        for drug_hgt_id in all_drug_hgt_ids:\n",
    "            for disease_hgt_id in all_disease_hgt_ids:\n",
    "                if (drug_hgt_id, disease_hgt_id) not in known_indications_hgt_ids: candidate_drug_ids.append(drug_hgt_id); candidate_disease_ids.append(disease_hgt_id)\n",
    "            count += 1\n",
    "            if count % 100 == 0: print(f\"  Generated pairs for {count}/{num_drugs} drugs...\")\n",
    "        print(f\"Generated {len(candidate_drug_ids)} candidate pairs.\")\n",
    "        if candidate_drug_ids: predicted_scores, predicted_pairs_hgt_ids = predict_scores(candidate_drug_ids, candidate_disease_ids)\n",
    "        else: predicted_scores, predicted_pairs_hgt_ids = None, None\n",
    "    else: predicted_scores, predicted_pairs_hgt_ids = None, None\n",
    "\n",
    "    # --- Rank and Display Candidates ---\n",
    "    if predicted_scores is not None and predicted_pairs_hgt_ids is not None:\n",
    "        print(\"\\nRanking candidates by predicted score...\"); scores_np = predicted_scores.numpy(); sorted_indices = np.argsort(scores_np)[::-1]\n",
    "        results = [];\n",
    "        for i in range(min(len(sorted_indices), NUM_CANDIDATES_TO_SHOW * 2)):\n",
    "            idx = sorted_indices[i]; drug_hgt_id, disease_hgt_id = predicted_pairs_hgt_ids[idx]\n",
    "            drug_name = hgt_id_to_drug_name.get(drug_hgt_id, f\"ID:{drug_hgt_id}\"); disease_name = hgt_id_to_disease_name.get(disease_hgt_id, f\"ID:{disease_hgt_id}\")\n",
    "            score = scores_np[idx]; results.append({\"Rank\": len(results) + 1, \"Drug Name\": drug_name, \"Disease Name\": disease_name, \"Predicted Score (Logit)\": score, \"Drug HGT ID\": drug_hgt_id, \"Disease HGT ID\": disease_hgt_id})\n",
    "        results_df = pd.DataFrame(results); print(f\"\\n--- Top {NUM_CANDIDATES_TO_SHOW} Drug Repurposing Candidates ---\"); print(results_df.head(NUM_CANDIDATES_TO_SHOW).to_markdown(index=False))\n",
    "        results_save_path = os.path.join(OUT_DIR, \"repurposing_candidates.csv\");\n",
    "        try: results_df.to_csv(results_save_path, index=False, float_format='%.4f'); print(f\"\\nSaved top candidates to: {results_save_path}\")\n",
    "        except Exception as e: print(f\"\\nError saving results to CSV: {e}\")\n",
    "    else: print(\"\\nCould not generate predictions.\")\n",
    "\n",
    "print(\"\\n--- Candidate Generation Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "MPHGT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
